{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "import pickle\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Basic text cleaning function\n",
    "def clean_text(text):\n",
    "    if pd.isna(text):\n",
    "        return \"missing\"\n",
    "    text = re.sub(r'\\W+', ' ', text)  # Remove special characters\n",
    "    text = text.lower().strip()  # Convert to lowercase and strip whitespace\n",
    "    return text\n",
    "\n",
    "def clean_text(text):\n",
    "    # Check if the input is a string before applying regex\n",
    "    if not isinstance(text, str):\n",
    "        return \"missing\"  # Return a default value for non-string input (e.g., NaN, numbers)\n",
    "\n",
    "    # Now apply your cleaning logic\n",
    "    text = re.sub(r'\\W+', ' ', text)  # Remove special characters\n",
    "    text = text.lower().strip()  # Convert to lowercase and strip whitespace\n",
    "    return text\n",
    "\n",
    "def extract_tables(file_path, sheet_name, check_names, check_idx, start_col, end_col):\n",
    "    \"\"\"\n",
    "    Extract subtables from a sheet using pandas DataFrame.\n",
    "    Subtables are separated by empty rows.\n",
    "    Only subtables where the header in index 2 (column 3) contains 'Movimiento', \n",
    "    'Cta Cte USD', 'TC Nacional', or 'TC Internacional' will be added to the list of subtables.\n",
    "    \"\"\"\n",
    "\n",
    "    # Read the Excel sheet into a DataFrame (only the first `col_num` columns)\n",
    "    df = pd.read_excel(file_path, sheet_name=sheet_name, usecols=range(start_col, end_col), engine=\"openpyxl\", header=None)\n",
    "    \n",
    "    # Replace all-NaN rows with a marker\n",
    "    df[\"is_empty\"] = df.isnull().all(axis=1)\n",
    "    \n",
    "    # Split the DataFrame into subtables based on empty rows\n",
    "    subtables = []\n",
    "    current_table = []\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        if row[\"is_empty\"]:\n",
    "            # If we encounter an empty row, save the current table (if not empty)\n",
    "            if current_table:\n",
    "                # Convert current table to DataFrame\n",
    "                table = pd.DataFrame(current_table).reset_index(drop=True)\n",
    "\n",
    "                # Find the row where the column index 2 matches one of the check_names\n",
    "                header_row_index = None\n",
    "                for i, row in table.iterrows():\n",
    "                    value = str(row[check_idx]).strip()\n",
    "                    if value in check_names:\n",
    "                        header_row_index = i\n",
    "                        break\n",
    "\n",
    "                if header_row_index is not None:\n",
    "                    # Set the identified row as the header and drop all rows above it\n",
    "                    table.columns = table.iloc[header_row_index]  # Set the header\n",
    "                    table = table[header_row_index + 1:].reset_index(drop=True)  # Drop rows above the header\n",
    "\n",
    "                    # Append the table if it's valid (it has content after setting the header)\n",
    "                    if not table.empty:\n",
    "                        subtables.append(table)\n",
    "\n",
    "                current_table = []  # Reset for the next table\n",
    "        else:\n",
    "            # Add non-empty rows to the current table\n",
    "            current_table.append(row[:-1])  # Exclude the \"is_empty\" column\n",
    "\n",
    "    # Add the last table if present\n",
    "    if current_table:\n",
    "        # Convert current table to DataFrame\n",
    "        table = pd.DataFrame(current_table).reset_index(drop=True)\n",
    "\n",
    "        # Find the row where the column index 2 matches one of the check_names\n",
    "        header_row_index = None\n",
    "        for i, row in table.iterrows():\n",
    "            value = str(row[check_idx]).strip()\n",
    "            if value in check_names:\n",
    "                header_row_index = i\n",
    "                break\n",
    "\n",
    "        if header_row_index is not None:\n",
    "            # Set the identified row as the header and drop all rows above it\n",
    "            table.columns = table.iloc[header_row_index]  # Set the header\n",
    "            table = table[header_row_index + 1:].reset_index(drop=True)  # Drop rows above the header\n",
    "\n",
    "            # Append the table if it's valid (it has content after setting the header)\n",
    "            if not table.empty:\n",
    "                subtables.append(table)\n",
    "\n",
    "    # Remove subtables where the first column is only NaN\n",
    "    subtables = [\n",
    "        table for table in subtables \n",
    "        if not table.iloc[:, 0].isna().all()  # Check if the first column is NOT all NaN\n",
    "    ]\n",
    "\n",
    "    # Ensure unique column names for all subtables at once\n",
    "    for idx, table in enumerate(subtables):\n",
    "        # Use pandas' built-in method to ensure unique column names\n",
    "        table.columns = [\n",
    "            f\"Unnamed_{i}\" if pd.isna(col) else col for i, col in enumerate(table.columns)\n",
    "        ]\n",
    "\n",
    "    return subtables\n",
    "\n",
    "def training_model(labeled_data, label_col_idx):\n",
    "    # Split into training and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(labeled_data['Combined_Text'], labeled_data.iloc[:, label_col_idx], test_size=0.2, random_state=42)\n",
    "    # TF-IDF vectorization\n",
    "    vectorizer = TfidfVectorizer(max_features=5000)  # Limit to top 5000 features\n",
    "    X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "    # Train the classifier\n",
    "    model = LogisticRegression(max_iter=1000, random_state=42)\n",
    "    model.fit(X_train_tfidf, y_train)\n",
    "    return model, vectorizer, X_train, X_test, y_test\n",
    "\n",
    "def classify_text(model, vectorizer, *args):\n",
    "    # Combine and clean input text\n",
    "    combined_text = ' '.join(clean_text(arg) for arg in args)\n",
    "    text_tfidf = vectorizer.transform([combined_text])  # Transform using the trained vectorizer\n",
    "    # Get predicted label and probabilities\n",
    "    label = model.predict(text_tfidf)[0]  # Predicted label\n",
    "    probabilities = model.predict_proba(text_tfidf)[0]  # Probabilities for all classes\n",
    "    # Get the confidence score for the predicted label\n",
    "    confidence_score = max(probabilities)  # Highest probability corresponds to the predicted label\n",
    "    return label, confidence_score\n",
    "\n",
    "# # Para Security leer desde el 0 hasta el 7\n",
    "# # Check index 1\n",
    "\n",
    "# # BCI 1 a 11\n",
    "# # Check index 3\n",
    "\n",
    "# # Example usage\n",
    "# file_path = \"./EERRs/2018/EERR Ene 18 rev0.xlsx\"\n",
    "# sheet_name = \"BCI \"\n",
    "# check_names = ['Movimiento', 'Cta Cte USD', 'TC Nacional', 'TC Internacional']\n",
    "# check_idx = 3\n",
    "# #check_names = ['Descripción']\n",
    "# end_col = 7\n",
    "# start_col = 0\n",
    "# tables = extract_tables(file_path, sheet_name, check_names, check_idx, start_col, end_col)\n",
    "# for i, table in enumerate(tables):\n",
    "#     print(table.iloc[:, 6].to_string())\n",
    "#     print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenamiento del modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Planilla BCI**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        precision    recall  f1-score   support\n",
      "\n",
      "      Comisión Expedia       0.00      0.00      0.00         1\n",
      "        Comisión otras       0.91      1.00      0.95        30\n",
      "Gastos Admininstración       0.00      0.00      0.00         2\n",
      "\n",
      "              accuracy                           0.91        33\n",
      "             macro avg       0.30      0.33      0.32        33\n",
      "          weighted avg       0.83      0.91      0.87        33\n",
      "\n"
     ]
    }
   ],
   "source": [
    "directory = \"./EERRs\"\n",
    "sheet_name = \"BCI \"\n",
    "tabla_dict = {'C85' : 0, 'CUSD' : 1, 'TCN' : 2, 'TCI' : 3}\n",
    "tabla = 'TCI'\n",
    "indx_tab = tabla_dict[tabla]\n",
    "\n",
    "label_col_indx = 7\n",
    "check_names = ['Movimiento', 'Cta Cte USD', 'TC Nacional', 'TC Internacional']\n",
    "check_idx = 3\n",
    "start_col, end_col = 1, 11\n",
    "\n",
    "df_list = []\n",
    "for root, dirs, files in os.walk(directory):\n",
    "    for filename in files:\n",
    "        if filename.endswith(\".xlsx\"):  # Only process .xlsx files\n",
    "            file_path = os.path.join(root, filename)  # Get the full file path\n",
    "            subtables = extract_tables(file_path, sheet_name, check_names, check_idx, start_col, end_col)  # Assuming `extract_tables` is defined\n",
    "            df = subtables[indx_tab]\n",
    "            # print(file_path)\n",
    "            # print(df.to_string())\n",
    "            # print()\n",
    "            df_list.append(df)  # Append the dataframe to the list\n",
    "data = pd.concat(df_list, ignore_index=True)\n",
    "labeled_data = data.dropna(subset=[data.columns[label_col_indx]])\n",
    "#print(labeled_data.to_string())\n",
    "\n",
    "# Procesar texto ---\n",
    "# Procesar texto ---\n",
    "feature_index_list = [2, 8, 9] # Index de las columnas que contienen el feature para entrenar al modelo. En este caso Movimiento, Fact, Prove, etc.\n",
    "#feature_index_list = [2] # Para CUSD, TCN, TCI\n",
    "\n",
    "# Clean the text columns\n",
    "\n",
    "# Clean the specified columns\n",
    "for col_index in feature_index_list:\n",
    "    labeled_data.iloc[:, col_index] = labeled_data.iloc[:, col_index].apply(clean_text)\n",
    "\n",
    "# Combine the specified columns into a single feature\n",
    "labeled_data['Combined_Text'] = labeled_data.iloc[:, feature_index_list].agg(' '.join, axis=1)\n",
    "\n",
    "model, vectorizer, X_train, X_test, y_test = training_model(labeled_data, label_col_idx=label_col_indx)\n",
    "\n",
    "# Evaluación del modelo\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = vectorizer.transform(X_test)\n",
    "y_pred = model.predict(X_test_tfidf)\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# --- SAVE THE MODEL AND VECTORIZER ---\n",
    "with open(f\"./modelo_{tabla}.pkl\", \"wb\") as model_file, open(f\"./vectorizer_{tabla}.pkl\", \"wb\") as vectorizer_file:\n",
    "    pickle.dump(model, model_file)  # Save the model\n",
    "    pickle.dump(vectorizer, vectorizer_file)  # Save the vectorizer\n",
    "\n",
    "#print(labeled_data.to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Planilla Security**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                 precision    recall  f1-score   support\n",
      "\n",
      "                   Comisión Bco       1.00      1.00      1.00        15\n",
      "               Gastos Operación       0.00      0.00      0.00         2\n",
      "                     Goperación       0.00      0.00      0.00         1\n",
      "                      Impuestos       1.00      0.86      0.92        14\n",
      "                       Prestamo       1.00      1.00      1.00         3\n",
      "                        Retiros       0.80      1.00      0.89         8\n",
      "                        Sueldos       1.00      0.94      0.97        18\n",
      "Transbank deposito ventanilla $       0.79      0.92      0.85        12\n",
      "                 Transf Egresos       0.98      1.00      0.99        42\n",
      "                 Transf Ingreso       0.89      0.98      0.93       142\n",
      "              Ventas deposito $       1.00      0.24      0.38        17\n",
      "\n",
      "                       accuracy                           0.92       274\n",
      "                      macro avg       0.77      0.72      0.72       274\n",
      "                   weighted avg       0.91      0.92      0.90       274\n",
      "\n"
     ]
    }
   ],
   "source": [
    "directory = \"./EERRs\"\n",
    "sheet_name = \"Security\" \n",
    "indx_tab = 0 \n",
    "\n",
    "# Security 0, 7\n",
    "# Check index 1\n",
    "# Security index Calve: 6\n",
    "label_col_indx = 6\n",
    "check_names = ['Descripción']\n",
    "check_idx = 1\n",
    "start_col, end_col = 0, 7\n",
    "\n",
    "df_list = []\n",
    "for root, dirs, files in os.walk(directory):\n",
    "    for filename in files:\n",
    "        if filename.endswith(\".xlsx\"):  # Only process .xlsx files\n",
    "            file_path = os.path.join(root, filename)  # Get the full file path\n",
    "            subtables = extract_tables(file_path, sheet_name, check_names, check_idx, start_col, end_col)  # Assuming `extract_tables` is defined\n",
    "            df = subtables[indx_tab]\n",
    "            df_list.append(df)  # Append the dataframe to the list\n",
    "data = pd.concat(df_list, ignore_index=True)\n",
    "labeled_data = data.dropna(subset=[data.columns[label_col_indx]])\n",
    "\n",
    "# Procesar texto ---\n",
    "feature_index_list = [1]\n",
    "# Clean the text columns\n",
    "# Clean the specified columns\n",
    "for col_index in feature_index_list:\n",
    "    labeled_data.iloc[:, col_index] = labeled_data.iloc[:, col_index].apply(clean_text)\n",
    "\n",
    "# Combine the specified columns into a single feature\n",
    "labeled_data['Combined_Text'] = labeled_data.iloc[:, feature_index_list].agg(' '.join, axis=1)\n",
    "\n",
    "model, vectorizer, X_train, X_test, y_test = training_model(labeled_data, label_col_idx=label_col_indx)\n",
    "\n",
    "# Evaluación del modelo\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = vectorizer.transform(X_test)\n",
    "y_pred = model.predict(X_test_tfidf)\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# --- SAVE THE MODEL AND VECTORIZER ---\n",
    "with open(\"./modelo_security.pkl\", \"wb\") as model_file, open(\"./vectorizer_security.pkl\", \"wb\") as vectorizer_file:\n",
    "    pickle.dump(model, model_file)  # Save the model\n",
    "    pickle.dump(vectorizer, vectorizer_file)  # Save the vectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cargar modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- LOAD THE MODEL AND VECTORIZER ---\n",
    "with open(\"logistic_model.pkl\", \"rb\") as model_file, open(\"tfidf_vectorizer.pkl\", \"rb\") as vectorizer_file:\n",
    "    loaded_clf = pickle.load(model_file)  # Load the model\n",
    "    loaded_vectorizer = pickle.load(vectorizer_file)  # Load the vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.float64'>\n"
     ]
    }
   ],
   "source": [
    "mov = ' Cargo Cuenta BCI'\n",
    "fact = 'Fact 2499,2753,2587,2695'\n",
    "prove = 'MIGUEL ANGEL NIEVAS                          '\n",
    "label, score = classify_text(model, vectorizer, mov, fact, prove)\n",
    "print(type(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
